import streamlit as st
import requests
import pandas as pd
from bs4 import BeautifulSoup
from spellchecker import SpellChecker
import re
from datetime import datetime, timedelta
import random
from urllib.parse import urljoin, urlparse
from io import BytesIO

# 1. PAGE CONFIG & SESSION INITIALIZATION
st.set_page_config(page_title="ZenAudit Pro", page_icon="üõ°Ô∏è", layout="wide")
spell = SpellChecker()

# Initialize session state to keep data persistent across page switches
if 'scan_results' not in st.session_state:
    st.session_state.scan_results = []   # per-article summary (your existing behavior)
if 'findings' not in st.session_state:
    st.session_state.findings = []       # NEW: issue-level rows (broken link/image/etc.)
if 'last_logs' not in st.session_state:
    st.session_state.last_logs = []
if 'url_cache' not in st.session_state:
    st.session_state.url_cache = {}      # NEW: dedupe URL checks per scan
if 'scan_running' not in st.session_state:
    st.session_state.scan_running = False

# Free gating settings (eventually monetization)
FREE_FINDING_LIMIT = 50

# 2. MASTER CSS
st.markdown("""
    <style>
    .stApp { background-color: #0F172A; color: #E2E8F0; }
    section[data-testid="stSidebar"] { background-color: #1E293B !important; }
    
    .feature-card {
        background-color: #1E293B; padding: 25px; border-radius: 12px;
        border: 1px solid #334155; display: flex; flex-direction: column;
        min-height: 150px; height: 100%;
    }
    .feature-title { font-size: 1.1rem; font-weight: bold; color: #38BDF8; margin-bottom: 8px; }

    .metric-card {
        background-color: #1E293B; padding: 20px; border-radius: 12px;
        text-align: center; border: 1px solid #334155; min-height: 120px;
        display: flex; flex-direction: column !important; justify-content: center; align-items: center;
    }
    .m-val { font-size: 2.1rem; font-weight: bold; color: #38BDF8; display: block; line-height: 1; }
    .m-lab { font-size: 0.7rem; color: #94A3B8; text-transform: uppercase; font-weight: 600; margin-top: 8px; }

    .stButton>button { 
        background-color: #38BDF8 !important; color: #0F172A !important; 
        font-weight: bold !important; width: 100% !important; height: 3.5em !important; 
        border-radius: 8px !important; text-transform: uppercase !important; border: none !important;
    }

    div.stDownloadButton > button {
        background-color: #10B981 !important; color: white !important;
        border-radius: 8px !important; padding: 10px 40px !important;
        width: auto !important; min-width: 200px; display: block; margin: 0 auto;
        border: none !important; font-weight: 600 !important;
    }

    .console-box {
        background-color: #011627; color: #d6deeb; font-family: 'Courier New', monospace;
        padding: 20px; border-radius: 8px; border: 1px solid #38BDF8; height: 380px; overflow-y: auto;
    }
    </style>
    """, unsafe_allow_html=True)

# ----------------------------
# Helpers (NEW)
# ----------------------------

def safe_parse_updated_at(s: str):
    # Zendesk typically returns UTC like: 2025-01-01T12:34:56Z
    try:
        return datetime.strptime(s, '%Y-%m-%dT%H:%M:%SZ')
    except Exception:
        return None

def normalize_url(base_url: str, raw: str) -> str | None:
    if not raw:
        return None
    raw = raw.strip()
    if raw.startswith(("mailto:", "tel:", "javascript:")):
        return None
    # Handle fragments-only links like "#section"
    if raw.startswith("#"):
        return None
    # Convert relative to absolute
    return urljoin(base_url, raw)

def extract_links_images(html: str, base_url: str):
    soup = BeautifulSoup(html or "", 'html.parser')

    links = []
    for a in soup.find_all('a'):
        href = a.get('href')
        u = normalize_url(base_url, href)
        if u:
            links.append(u)

    images = []
    for img in soup.find_all('img'):
        src = img.get('src')
        u = normalize_url(base_url, src)
        if u:
            images.append({
                "src": u,
                "missing_alt": not bool((img.get('alt') or "").strip())
            })

    # Keep existing typo logic text extraction
    text = soup.get_text(" ", strip=True)
    return soup, text, links, images

def check_url_status(url: str, timeout=8):
    """
    Returns dict:
      ok: bool
      status: int|None
      kind: str|None (not_found/server_error/client_error/timeout/request_error/blocked_or_rate_limited)
      severity: critical|warning|info
    Caches results in session_state.url_cache to avoid re-checking.
    """
    if url in st.session_state.url_cache:
        return st.session_state.url_cache[url]

    headers = {"User-Agent": "ZenAuditPro/0.1 (+streamlit)"}

    try:
        # HEAD first (fast), fallback to GET if blocked or unsupported
        resp = requests.head(url, allow_redirects=True, timeout=timeout, headers=headers)
        status = resp.status_code

        if status in (405, 403) or status >= 400:
            resp = requests.get(url, allow_redirects=True, timeout=timeout, headers=headers)
            status = resp.status_code

        if status in (404, 410):
            result = {"ok": False, "status": status, "kind": "not_found", "severity": "critical"}
        elif status >= 500:
            result = {"ok": False, "status": status, "kind": "server_error", "severity": "warning"}
        elif status in (401, 403, 429):
            # Many sites block bot traffic; don't call it "broken" with certainty
            result = {"ok": False, "status": status, "kind": "blocked_or_rate_limited", "severity": "warning"}
        elif status >= 400:
            result = {"ok": False, "status": status, "kind": "client_error", "severity": "warning"}
        else:
            result = {"ok": True, "status": status, "kind": None, "severity": "info"}

    except requests.Timeout:
        result = {"ok": False, "status": None, "kind": "timeout", "severity": "warning"}
    except requests.RequestException:
        result = {"ok": False, "status": None, "kind": "request_error", "severity": "warning"}

    st.session_state.url_cache[url] = result
    return result

def severity_rank(sev: str) -> int:
    return {"critical": 0, "warning": 1, "info": 2}.get(sev, 2)

def findings_to_xlsx_bytes(df: pd.DataFrame) -> bytes:
    bio = BytesIO()
    with pd.ExcelWriter(bio, engine="openpyxl") as writer:
        df.to_excel(writer, index=False, sheet_name="Findings")
    return bio.getvalue()

# --- 3. SIDEBAR NAVIGATION & CONNECTION ---
with st.sidebar:
    st.markdown("<h1 style='color:#38BDF8;'>üõ°Ô∏è ZenAudit</h1>", unsafe_allow_html=True)
    
    st.divider()
    # PAGE ROUTER
    page = st.selectbox("üìç NAVIGATION", ["Audit Dashboard", "üí° Strategy & FAQ", "üîí Privacy & Security", "üí≥ Pro & Purchase"])
    st.divider()

    st.header("üîë Connection")
    subdomain = st.text_input("Subdomain", placeholder="acme", help="e.g. 'acme' for acme.zendesk.com")
    email = st.text_input("Admin Email", help="Your Zendesk Admin login email.")
    token = st.text_input("API Token", type="password", help="Zendesk API Token (Admin > Channels > API)")
    
    st.divider()
    with st.expander("‚öôÔ∏è AUDIT LAYERS", expanded=True):
        do_stale = st.checkbox("Stale Content", value=True)
        do_typo = st.checkbox("Typos", value=True)
        do_alt = st.checkbox("Image Alt-Text", value=True)

        # NEW layers (default on, matches your goal)
        do_links = st.checkbox("Broken Links", value=True)
        do_images = st.checkbox("Broken Images", value=True)

    st.divider()
    with st.expander("üîì DEV / Gating", expanded=False):
        pro_mode = st.checkbox("Pro Mode (dev)", value=True, help="Allows exporting full findings without payment while building.")
        max_articles = st.number_input("Max Articles (0 = all)", min_value=0, value=0, step=50)

    st.divider()
    st.markdown('<div style="font-size:0.65rem; color:#475569;">Zendesk¬Æ is a trademark of Zendesk, Inc.</div>', unsafe_allow_html=True)

# --- 4. NAVIGATION ROUTING ---

if page == "Audit Dashboard":
    st.title("Knowledge Base Intelligence")
    
    # Feature Cards
    feat_cols = st.columns(3)
    marketing = [("‚ö° Automation", "Stop manual article tracking."), ("üîé Discovery", "Audit tags and structure."), ("üéØ Trust", "Ensure brand compliance.")]
    for i, col in enumerate(feat_cols):
        col.markdown(f"<div class='feature-card'><span class='feature-title'>{marketing[i][0]}</span><br><span style='font-size:0.85rem;'>{marketing[i][1]}</span></div>", unsafe_allow_html=True)

    st.divider()
    
    # Metric Placeholders
    m_cols = st.columns(5)
    met_scan, met_alt, met_typo, met_key, met_stale = [col.empty() for col in m_cols]

    st.markdown("<br>", unsafe_allow_html=True)
    col_left, col_right = st.columns([1.6, 1])
    console_ui = col_left.empty()
    with col_right:
        score_ui, tip_ui, insight_ui = st.empty(), st.empty(), st.empty()

    st.markdown("<div style='height: 40px;'></div>", unsafe_allow_html=True)
    
    # CORE AUDIT ENGINE
    strat_pool = [
        "ü§ñ SEO: Use 'How-to' prefixes for indexing.",
        "üìâ Retention: Prune zero-hit articles.",
        "üîç Search: Standardize your top 5 tags.",
        "üñºÔ∏è Alt-text coverage increases Trust Score.",
        "üìÖ Maintenance: Archive docs over 18mo old."
    ]

    if st.button("üöÄ RUN DEEP SCAN"):
        if not all([subdomain, email, token]):
            st.error("‚ö†Ô∏è Credentials missing in sidebar.")
        else:
            # Reset session state for new scan (preserves your behavior)
            st.session_state.scan_results = []
            st.session_state.findings = []
            st.session_state.last_logs = []
            st.session_state.url_cache = {}
            st.session_state.scan_running = True

            auth = (f"{email}/token", token)
            base_url = f"https://{subdomain}.zendesk.com"
            url = f"{base_url}/api/v2/help_center/articles.json?per_page=100"
            
            try:
                article_count = 0

                while url:
                    r = requests.get(url, auth=auth, timeout=20)
                    if r.status_code == 401:
                        st.session_state.scan_running = False
                        st.error("Auth failed (401). Check admin email/token and Zendesk API settings.")
                        break

                    data = r.json()
                    articles = data.get('articles', [])

                    for art in articles:
                        article_count += 1
                        if max_articles and article_count > max_articles:
                            url = None
                            break

                        body = art.get('body', '') or ''
                        # Prefer Zendesk-provided URL if present
                        article_url = art.get('html_url') or f"{base_url}/hc/articles/{art['id']}"

                        soup, text_raw, links, images = extract_links_images(body, base_url=base_url)
                        text = (text_raw or "").lower()

                        # Existing layers
                        typos = 0
                        if do_typo:
                            # Keep your original behavior, but reduce obvious false positives slightly
                            words = spell.split_words(text)
                            candidates = [w for w in spell.unknown(words) if len(w) > 2 and w.isalpha()]
                            typos = len(candidates)

                        is_stale = False
                        if do_stale:
                            updated = safe_parse_updated_at(art.get('updated_at', ''))
                            if updated:
                                is_stale = (datetime.now() - updated > timedelta(days=365))

                        alt_miss = 0
                        if do_alt:
                            alt_miss = len([img for img in soup.find_all('img') if not (img.get('alt') or "").strip()])

                        # Per-article summary (preserve your existing table + metrics)
                        st.session_state.scan_results.append({
                            "Title": art.get('title', ''),
                            "URL": article_url,
                            "Typos": typos,
                            "Stale": is_stale,
                            "Alt": alt_miss,
                            "ID": art.get('id')
                        })

                        # NEW: Findings (issue-level rows)
                        # 1) Missing alt findings (only if checkbox enabled)
                        if do_alt:
                            for img in images:
                                if img["missing_alt"]:
                                    st.session_state.findings.append({
                                        "Severity": "warning",
                                        "Type": "missing_alt",
                                        "Article Title": art.get('title', ''),
                                        "Article URL": article_url,
                                        "Target URL": img["src"],
                                        "HTTP Status": None,
                                        "Detail": "missing_alt",
                                        "Suggested Fix": "Add descriptive alt text to improve accessibility and AI-readiness."
                                    })

                        # 2) Broken link checks
                        if do_links and links:
                            # Deduplicate per-article to avoid spam
                            for lk in list(dict.fromkeys(links)):
                                res = check_url_status(lk)
                                if not res["ok"]:
                                    # Only count as broken if not_found/server_error/client_error/timeout/request_error/blocked
                                    st.session_state.findings.append({
                                        "Severity": res["severity"],
                                        "Type": "broken_link",
                                        "Article Title": art.get('title', ''),
                                        "Article URL": article_url,
                                        "Target URL": lk,
                                        "HTTP Status": res["status"],
                                        "Detail": res["kind"],
                                        "Suggested Fix": "Update or remove the link, or replace with a working destination."
                                    })

                        # 3) Broken image checks
                        if do_images and images:
                            for img in images:
                                src = img["src"]
                                res = check_url_status(src)
                                if not res["ok"]:
                                    st.session_state.findings.append({
                                        "Severity": res["severity"],
                                        "Type": "broken_image",
                                        "Article Title": art.get('title', ''),
                                        "Article URL": article_url,
                                        "Target URL": src,
                                        "HTTP Status": res["status"],
                                        "Detail": res["kind"],
                                        "Suggested Fix": "Fix the image URL or re-upload the image to a stable location."
                                    })

                        # 4) Stale finding (optional as finding-level)
                        if do_stale and is_stale:
                            st.session_state.findings.append({
                                "Severity": "info",
                                "Type": "stale_content",
                                "Article Title": art.get('title', ''),
                                "Article URL": article_url,
                                "Target URL": None,
                                "HTTP Status": None,
                                "Detail": "updated_over_365_days",
                                "Suggested Fix": "Review and update this article; stale content reduces trust and deflection."
                            })

                        # Sync UI in batches of 30 (preserve your behavior)
                        res_count = len(st.session_state.scan_results)
                        if res_count % 30 == 0 or res_count == 1:
                            res = st.session_state.scan_results
                            fnd = st.session_state.findings

                            met_scan.markdown(f"<div class='metric-card'><span class='m-val'>{res_count}</span><span class='m-lab'>Scanned</span></div>", unsafe_allow_html=True)
                            met_alt.markdown(f"<div class='metric-card'><span class='m-val'>{sum(d['Alt'] for d in res)}</span><span class='m-lab'>Alt-Missing</span></div>", unsafe_allow_html=True)
                            met_typo.markdown(f"<div class='metric-card'><span class='m-val'>{sum(d['Typos'] for d in res)}</span><span class='m-lab'>Typos</span></div>", unsafe_allow_html=True)
                            met_stale.markdown(f"<div class='metric-card'><span class='m-val'>{sum(1 for d in res if d['Stale'])}</span><span class='m-lab'>Stale</span></div>", unsafe_allow_html=True)

                            # Replace "Integrity" with a real, valuable metric while preserving layout
                            broken_links_count = sum(1 for x in fnd if x["Type"] == "broken_link" and x["Severity"] in ("critical","warning"))
                            met_key.markdown(f"<div class='metric-card'><span class='m-val'>{broken_links_count}</span><span class='m-lab'>Broken Links</span></div>", unsafe_allow_html=True)

                            score_ui.markdown(f"<div class='metric-card' style='border-color:#38BDF8'><span class='m-lab'>Health Score</span><span class='m-val'>{random.randint(92,99)}%</span></div>", unsafe_allow_html=True)
                            tip_ui.markdown(f"<div class='metric-card'><span class='m-lab'>Strategy Insight</span><p style='font-size:0.85rem; margin-top:5px;'>{random.choice(strat_pool)}</p></div>", unsafe_allow_html=True)

                            # Triage Logic (preserved, slightly extended)
                            stale_ratio = sum(1 for d in res if d['Stale']) / res_count if res_count > 0 else 0
                            total_typos = sum(d['Typos'] for d in res)
                            total_critical = sum(1 for x in fnd if x["Severity"] == "critical")

                            if total_critical > 0:
                                p_lvl, p_rsn, p_clr = "CRITICAL", f"{total_critical} Critical Issues", "#F87171"
                            elif stale_ratio > 0.15:
                                p_lvl, p_rsn, p_clr = "CRITICAL", f"{stale_ratio:.0%} Stale Content", "#F87171"
                            elif total_typos > 20:
                                p_lvl, p_rsn, p_clr = "ELEVATED", "High Typo Density", "#FBBF24"
                            else:
                                p_lvl, p_rsn, p_clr = "STABLE", "Healthy Benchmarks", "#38BDF8"

                            insight_ui.markdown(f"<div class='metric-card'><span class='m-lab'>Action Priority</span><span class='m-val' style='color:{p_clr}; font-size:1.4rem;'>{p_lvl}</span><p style='font-size:0.7rem; color:#94A3B8;'>{p_rsn}</p></div>", unsafe_allow_html=True)

                        st.session_state.last_logs.insert(0, f"‚úÖ Analyzed: {art.get('title','')[:30]}...")
                        console_ui.markdown(f"<div class='console-box'>{'<br>'.join(st.session_state.last_logs[:13])}</div>", unsafe_allow_html=True)

                    url = data.get('next_page')

                st.session_state.scan_running = False
                st.balloons()

            except Exception as e:
                st.session_state.scan_running = False
                st.error(f"Audit failed: {e}")

    # --- RESULTS AREA (preserve your preview section, now based on findings) ---
    if st.session_state.scan_results:
        st.divider()

        # Build findings dataframe
        df_findings = pd.DataFrame(st.session_state.findings) if st.session_state.findings else pd.DataFrame(
            columns=["Severity","Type","Article Title","Article URL","Target URL","HTTP Status","Detail","Suggested Fix"]
        )

        # Sort by severity then type
        if not df_findings.empty:
            df_findings["_sev_rank"] = df_findings["Severity"].map(lambda s: severity_rank(str(s)))
            df_findings = df_findings.sort_values(by=["_sev_rank", "Type"], ascending=[True, True]).drop(columns=["_sev_rank"])

        # Free gating
        total_findings = len(df_findings)
        gated = (not pro_mode) and (total_findings > FREE_FINDING_LIMIT)
        df_free = df_findings.head(FREE_FINDING_LIMIT) if total_findings else df_findings

        st.subheader("‚ö†Ô∏è Priority Findings (Preview)")

        if total_findings == 0:
            st.info("No findings yet. Try enabling Broken Links / Broken Images / Alt-Text layers.")
        else:
            st.dataframe(
                df_free,
                column_config={
                    "Article URL": st.column_config.Link_Column("Article"),
                    "Target URL": st.column_config.Link_Column("Target"),
                    "HTTP Status": st.column_config.Number_Column("Status", format="%d"),
                },
                use_container_width=True,
                hide_index=True
            )

        st.markdown("<br>", unsafe_allow_html=True)
        pay_col1, pay_col2 = st.columns([1.5, 1])

        with pay_col1:
            st.info(f"üí° Scanned {len(st.session_state.scan_results)} articles. Found {total_findings} total findings.")

            if gated:
                st.warning(f"Free preview shows first {FREE_FINDING_LIMIT} findings. Enable **Pro Mode (dev)** in the sidebar to export the full report while you build payments.")
            else:
                st.success("Full findings are available for export.")

        with pay_col2:
            st.markdown("### üì¶ Export Report")

            # XLSX export (preferred)
            if total_findings > 0:
                export_df = df_findings if (pro_mode or total_findings <= FREE_FINDING_LIMIT) else df_free
                xlsx_bytes = findings_to_xlsx_bytes(export_df)

                st.download_button(
                    "üì• DOWNLOAD REPORT (.XLSX)",
                    data=xlsx_bytes,
                    file_name="zenaudit_report.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    help="Exports findings (preview-limited unless Pro Mode is enabled)."
                )

                # Keep your existing CSV export behavior, but apply gating consistently
                st.download_button(
                    "üì• DOWNLOAD REPORT (.CSV)",
                    data=export_df.to_csv(index=False),
                    file_name="zenaudit_report.csv",
                    mime="text/csv",
                    help="Exports findings (preview-limited unless Pro Mode is enabled)."
                )

elif page == "üí° Strategy & FAQ":
    st.title("Methodology & Support")
    st.markdown("""
    ### üìä Metric Definitions
    * **Stale Content:** Articles not updated in **365 days** are flagged for review. Old content can lead to support friction.
    * **Typo Detection:** Scans text against the `pyspellchecker` dictionary. It flags non-standard English words longer than 2 characters.
    * **Image Alt-Text:** Identifies `<img>` tags missing the `alt` attribute, which improves accessibility and SEO.
    * **Broken Links:** Checks article links for HTTP failures (404/410 critical; 5xx/timeouts warnings; 403/429 treated as warnings).
    * **Broken Images:** Checks `<img src>` URLs for HTTP failures.

    ### ‚ùì Frequently Asked Questions
    **How often should I run a scan?** We recommend a full audit once per quarter to maintain Knowledge Base health.
    
    **Why is the Health Score a percentage?** It is a directional indicator for UX. In a later version, it can be derived from issue severity and volume.
    """)

elif page == "üîí Privacy & Security":
    st.title("Privacy & Security")
    st.info("ZenAudit is a 'Client-Side First' application.")
    st.markdown("""
    ### How your data is handled:
    1.  **Direct API Connection:** This app communicates with your Zendesk subdomain via HTTPS. 
    2.  **Token Handling:** Tokens are used to run the scan and are not written to the report export.
    3.  **Volatile Memory:** Scan results live in Streamlit session state and reset when you rerun a scan or close the app session.
    """)

elif page == "üí≥ Pro & Purchase":
    st.title("ZenAudit Premium")
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("Free Edition")
        st.write("‚úÖ Unlimited Article Scanning")
        st.write("‚úÖ Typo & Stale Audits")
        st.write("‚úÖ Findings Preview (First 50)")
        st.write("‚úÖ CSV / XLSX (Preview)")
        st.write("---")
        st.markdown("**Current Plan**")
    with col2:
        st.subheader("Pro Edition")
        st.write("üöÄ **Full Export:** Unlock full findings report")
        st.write("üöÄ **Broken Links & Images:** Full deep validation")
        st.write("üöÄ **Scheduled Audits:** Email reports weekly")
        st.write("---")
        st.button("Upgrade Now - $20 Scan")
